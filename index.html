<!DOCTYPE html><html lang="en"><head><title>Ismail's Web Corner</title><link rel="stylesheet" href=https://latex.now.sh/style.min.css /></head><body><h1>Ismail Elezi, Ph.D.</h1><img src="img/8701777.jpeg" width="200" align="left" style="padding:10px;"><p>Hi, I'm Ismail. I am a Principal Research Scientist at Huawei Noah's Ark in London, working with <a href="https://jiankangdeng.github.io/">Jiankang Deng</a>, where I am leading one of the multi-modality learning teams. <br><br> 
After finishing my bachelor studies at the University of Prishtina, Kosovo, I obtained both my master's and Ph.D. degree at Ca' Foscari University of Venice, Italy, advised by professors <a href="https://www.dsi.unive.it/~pelillo/">Marcello Pelillo</a> and <a href="https://www.zhaw.ch/en/about-us/person/stdm/">Thilo Stadelmann</a>. Then, I worked as a postdoctoral researcher with professor <a href="https://dvl.in.tum.de/team/lealtaixe/">Laura Leal-Taixé </a> at Technical University of Munich and interned with <a href="https://alvarezlopezjosem.github.io/">Jose M. Alvarez</a> at NVIDIA, Santa Clara.</p><br>

If you have a first author paper in a top ML/CV conference, and want to do an internship with me, please send me an email (find it in my CV). My interns/mentees usually author papers with me and go on to great companies or PhD programs.
	
<b>
<p>
<a href="https://twitter.com/Ismail_Elezi">Twitter</a> 
	<a href="https://scholar.google.com/citations?user=tpaCLrsAAAAJ&hl=en">Scholar</a> 
	<a href="https://www.linkedin.com/in/ismail-elezi-33958b32/">Linkedin</a> 
	<a href="https://github.com/TheRevanchist">GitHub</a> 
	<a href="other/Ismail_Resume.pdf">CV</a></b> 
	
	<h2>News</h2><ul>
		<li><b><i>February 2025:</i></b> Got a <a href="https://arxiv.org/pdf/2410.11774">paper</a> accepted to CVPR 2025. Congratulations to Kostas. </li><li>

		<b><i>January 2025:</i></b> Finally got a <a href="https://arxiv.org/abs/2410.17174">paper</a> accepted to ICLR. Congratulations to my former intern Prannay and see you in Singapore </li><li>

		<b><i>September 2024:</i></b> One paper accepted to NeurIPS 2024. Congratulations to Roy for <a href="https://arxiv.org/pdf/2405.17991">VeLoRA</a>. See you in Vancouver. </li><li>
		
		<b><i>February 2024:</i></b> Two papers accepted to CVPR 2024. Congratulations to Prady for <a href="https://arxiv.org/abs/2403.00939">G3DR</a>  and Roy for <a href="https://arxiv.org/abs/2403.06213">VKD</a>! See you at Seattle.</li><li>

		<b><i>November 2023:</i></b> One <a href="https://arxiv.org/html/2312.15702v1">paper</a> got accepted to AAAI 2024. Congratulations to Chencheng!</li><li>

		<b><i>April 2023:</i></b> Started a new position as a Senior Research Scientist at Huawei Noah's Ark lab, working with Jiankang Deng. Very excited for the new work and for moving to London! </li><li>

		<b><i>February 2023:</i></b> One <a href="https://arxiv.org/abs/2206.04656">paper</a> got accepted to CVPR 2023. Congratulations to Jenny!</li><li>

		<b><i>September 2022:</i></b> Two papers accepted to NeurIPS 2022. Congratulations to Vlad and Peter. Happy to be back to the US after covid. </li>
		</ul>

	
	
	<h2>Students and Interns Supervised</h2><ul>
		<li>Yura Choi (Yonsei University, 2025) </li>
		<li>Ye-Bin Moon (POSTECH, 2025) </li>
		<li>Xin Wen (University of Hong Kong, 2024-2025) </li>
		<li>Aysim Toker (Technical Unversity of Munich, 2024-2025) </li>
		<li>Tatiana Gaintseva (Queen Mary University, 2024-2025) </li>
		<li>Bingchen Zhao (University of Edinburgh, 2024) </li>
		<li>Changrui Chen (Warvick University, 2024) <span>&#8594;</span> <i> Research Scientist at Huawei</i></li>
		<li>Prannay Kaul (University of Oxford, VGG, 2024)<span>&#8594;</span> <i> Applied Scientist at Amazon</i></li>
		<li>Roy Miles (Imperial College London, 2023) <span>&#8594;</span> <i> Research Scientist at Huawei</i></li>
		<li>Konstantinos Alexandridis (King's College London, 2023) <span>&#8594;</span> <i> Research Scientist at Huawei</i></li>
		<li>Yunqi Miao (Warwick University, 2023) <span>&#8594;</span> <i> Research Scientist at Huawei</i></li>
		<li>Chencheng Ma (Chinese Academy of Sciences, 2023) <span>&#8594;</span> <i>Research Scientist at Kunlun</i></li>
		<li>Volodymyr Fomenko (Master thesis at TUM, 2022) <span>&#8594;</span> <i>Technical Staff at OpenAI</i></li>
		<li>Jenny Seidenschwarz (Master thesis and Ph.D. student at TUM, 2020-2023) <span>&#8594;</span> <i>Ph.D. student at TUM</i></li>
		<li>Franziska Gerken (Ph.D. student at TUM, 2020-2023) <span>&#8594;</span> <i>Ph.D. student at TUM</i></li>
		<li>Peter Kocsis (Master thesis at TUM, 2022) <span>&#8594;</span> <i>Ph.D. student at TUM</i></li>
		<li>Peter Sukenik (Guided Research at TUM, 2021) <span>&#8594;</span> <i>Ph.D. student at  IST Austriat</i></li>
		<li>Feliks Hibraj (Research Intern at TUM, 2021) <span>&#8594;</span> <i>Computer vision software engineer at SNAP </i></li>
		<li>Laurin Wagner (Master thesis at TUM, 2020) <span>&#8594;</span> <i>Machine learnign software engineer myReha </i></li></ul>
			
		<h2>Service</h2><ul><li>Area Chair (AC) for WACV 2021.</li>
				<li>Reviewer for top conferences and journals in Computer Vision and Machine Learning: CVPR, ECCV, ICCV, NeurIPS, ICML, ICLR, IJCV, TMLR, etc. I even managed to get outstanding reviewer awards in CVPR 2021 and ICCV 2021</li>
				<li>I co-organized <a href="https://dvsml2022-tutorial.github.io/">Deep Visual Similarity and Metric Learning</a> tutorial in CVPR 2022.</li></ul> 
				
				
		<h2>Teaching</h2> 
				<ul><li><b><i>Summer 2022:</i></b> Lecturer for <a href="https://dvl.in.tum.de/teaching/cv3dst-ss22/">IN2375: Computer Vision III: Detection, Segmentation and Tracking (CV3DST)</a> (TU Munich)</li>
					<li><b><i>Summer 2022:</i></b> Lecturer for <a href="https://dvl.in.tum.de/teaching/i2dl-ss22/">IN2346: Introduction to Deep Learning (I2DL)</a> (TU Munich)</li>
					<li><b><i>Winter 2021:</i></b> Lecturer for <a href="https://dvl.in.tum.de/teaching/cv3dst-ss21/">IIN2375: Computer Vision III: Detection, Segmentation and Tracking (CV3DST)</a> (TU Munich)
					<li><b><i>Winter 2021:</i></b> Lecturer for <a href="https://dvl.in.tum.de/teaching/adl4cv-ws21/">IN2389: Advanced Deep Learning for Computer vision (ADL4CV) </a> (TU Munich)</li>
					<li><b><i>2019-2023:</i></b> Lecturer for Introduction to Deep Learning with PyTorch (Datacamp). Around 30K students took the course before it got retired in 2023.</li>
					<li>I have a few lectures online in  <a href="https://www.youtube.com/watch?v=1wANXzUISMU">Transformers</a>, <a href="https://www.youtube.com/watch?v=ihkylUbqFMI">Semi-Supervised Learning</a>, and <a href="https://www.youtube.com/watch?v=oYUkAvhBNsg">Active Learning</a>. They are a couple of years old so relatively outdated.</li> </ul>
					
						
		<h2>Selected Publications</h2> <div class="thebibliography">

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/fractal.PNG" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> K. Alexandridis, <b>I. Elezi</b>, J. Deng, A. Nguyen, S. Luo: <i>Fractal Calibration for long-tailed object detection</i>, Conference on Computer Vision and Pattern Recognition (CVPR), 2025.  </br> <a href="https://arxiv.org/pdf/2410.11774?" target="_blank">paper</a>  </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/iclr.PNG" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> P. Kaul, C. Ma, <b>I. Elezi</b>, J. Deng: <i>From Attention to Activation: Unraveling the Enigmas of Large Language Models</i>, International Conference on Learning Representations (ICLR), 2025.  </br> <a href="https://arxiv.org/pdf/2410.17174" target="_blank">paper</a>  </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/velora.png" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> R. Miles, P.Reddy, <b>I. Elezi</b>, J. Deng: <i>VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections</i>, Neural Information Processing Systems (NeurIPS), 2024.  </br> <a href="https://github.com/roymiles/VeLoRA" target="_blank">code</a> <a href="https://arxiv.org/pdf/2405.17991" target="_blank">paper</a>  </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/g3dr_teaser.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> P. Reddy*, <b>I. Elezi*</b>, J. Deng: <i>G3DR: Generative 3D Reconstruction in ImageNet</i>, Conference on Computer Vision and Pattern Recognition (CVPR), 2024.  </br> <a href="https://preddy5.github.io/g3dr_website/" target="_blank">page</a>  <a href="https://github.com/preddy5/G3DR" target="_blank">code</a> <a href="https://arxiv.org/abs/2403.00939" target="_blank">paper</a>  </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/vkd_teaser.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> R. Miles, <b>I. Elezi</b>, J. Deng: <i>VkD: Improving Knowledge Distillation using Orthogonal Projections</i>, Conference on Computer Vision and Pattern Recognition (CVPR), 2024. </br> <a href="https://github.com/roymiles/vkd" target="_blank">code</a> <a href="https://arxiv.org/abs/2403.06213" target="_blank">paper</a>  </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/three_heads.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> C. Ma, <b>I. Elezi</b>, J. Deng, W. Dong, C. Xu: <i>Three Heads Are Better Than One: Complementary Experts for Long-Tailed Semi-supervised Learning</i>, Conference on Artificial Intelligence (AAAI), 2024.  </br> <a href="https://github.com/machengcheng2016/CPE-LTSSL" target="_blank">code</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29334" target="_blank">paper</a>  </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/mot_teaser.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> J. Seidenschwarz, G. Braso, V. Serrano, <b>I. Elezi</b>, L. Leal-Taixé: <i>Simple Cues Lead to a Strong Multi-Object Tracker</i>, Conference on Computer Vision and Pattern Recognition (CVPR), 2023.  </br> <a href="https://github.com/dvl-tum/GHOST" target="_blank">code</a> <a href="https://arxiv.org/abs/2206.04656" target="_blank">paper</a>  <a href="https://www.youtube.com/watch?v=3gozhzOHwE0" target="_blank">video</a> </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/gl.PNG" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> <b>I. Elezi*</b>, J. Seidenschwarz*, L. Wagner*, S. Vascon, A. Torcinovich, M. Pelillo, L. Leal-Taixé: <i>The group loss++: A deeper look into group loss for deep metric learning</i>, Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2023.  </br> <a href="https://ieeexplore.ieee.org/abstract/document/9745769" target="_blank">paper</a>  </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/fomenko_neurips_2022.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> V. Fomenko, <b>I. Elezi</b>, D. Ramanan, L. Leal-Taixé, A. Ošep: <i>Learning to Discover and Detect Objects</i>, Neural Information Processing Systems (NeurIPS), 2022.  </br> <a href="https://vlfom.github.io/RNCDL/" target="_blank">page</a> <a href="https://drive.google.com/file/d/1gDwgv-nKM3btpCqPU98RdC7ztuHu2cYF/view" target="_blank">poster</a> <a href="https://www.youtube.com/watch?v=zWpUnXNplfQ" target="_blank">video</a> <a href="https://github.com/vlfom/RNCDL" target="_blank">code</a> <a href="https://arxiv.org/abs/2210.10774" target="_blank">paper</a>  </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/mlp.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> P. Kocsis, P. Sukenik, G. Braso, M. Niessner, L. Leal-Taixé, <b>I. Elezi</b>: <i>The Unreasonable Effectiveness of Fully-Connected Layers for Low-Data Regimes</i>, Neural Information Processing Systems (NeurIPS), 2022. </br> <a href="https://peter-kocsis.github.io/LowDataGeneralization/" target="_blank">page</a>  <a href="https://github.com/Peter-Kocsis/LowDataGeneralization" target="_blank">code</a> <a href="https://arxiv.org/pdf/2210.05657.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/alssl.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> <b>I. Elezi</b>, Z. Yu, A. Anandkumar, L. Leal-Taixé, J. Alvarez: <i>Not All Labels Are Equal:
			Rationalizing The Labeling Costs for Training Object Detection</i>, Conference on Computer Vision and Pattern Recognition (CVPR), 2022. </br> <a href="https://github.com/NVlabs/AL-SSL" target="_blank">code</a> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Elezi_Not_All_Labels_Are_Equal_Rationalizing_the_Labeling_Costs_for_CVPR_2022_paper.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/mixture.PNG" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> J. Choi, <b>I. Elezi</b>, H. Lee, C. Farabet, J. Alvarez: <i>Active Learning for Deep Object Detection via Probabilistic Modeling</i>, International Conference on Computer Vision (ICCV), 2021. </br> <a href="https://github.com/NVlabs/AL-MDN" target="_blank">code</a> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Choi_Active_Learning_for_Deep_Object_Detection_via_Probabilistic_Modeling_ICCV_2021_paper.pdf" target="_blank">paper</a>  <a href="https://www.youtube.com/watch?v=5XyLGiSQbjQ" target="_blank">video</a> </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/more_cub.png" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> J. Seidenschwarz, <b>I. Elezi</b>, L. Leal-Taixé: <i>Learning Intra-Batch Connections for Deep Metric Learning</i>, International Conference on Machine Learning (ICML), 2021. </br> <a href="https://github.com/dvl-tum/intra_batch" target="_blank">code</a> <a href="http://proceedings.mlr.press/v139/seidenschwarz21a/seidenschwarz21a.pdf" target="_blank">paper</a>  <a href="https://www.youtube.com/watch?v=PpZSGatETPs" target="_blank">video</a> </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/gl.PNG" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> <b>I. Elezi</b>, S. Vascon, A. Torcinovich, M. Pelillo, L. Leal-Taixé: <i>The Group Loss for Deep Metric Learning</i>, European Conference on Computer Vision (ECCV), 2020. </br> <a href="https://dvl.in.tum.de/blog/2019/12/04/GroupLoss.html" target="_blank">page</a>  <a href="https://github.com/dvl-tum/group_loss" target="_blank">code</a> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520273.pdf" target="_blank">paper</a>  <a href="https://www.youtube.com/watch?v=PVidY1O31yU" target="_blank">video</a> </p></div></div><br clear="all" />

		<div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/ciagan.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> M. Maximov*, <b>I. Elezi*</b>, L. Leal-Taixé: <i>CIAGAN: Conditional Identity Anonymization Generative Adversarial
			Networks</i>, Conference on Computer Vision and Pattern Recognition (CVPR), 2020.  </br>  <a href="https://github.com/dvl-tum/ciagan" target="_blank">code</a> <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Maximov_CIAGAN_Conditional_Identity_Anonymization_Generative_Adversarial_Networks_CVPR_2020_paper.pdf" target="_blank">paper</a>  <a href="https://www.youtube.com/watch?v=A6feoh9NLwQ" target="_blank">video</a> </p></div></div><br clear="all" />
